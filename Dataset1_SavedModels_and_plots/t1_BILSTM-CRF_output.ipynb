{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "a1w4YdHMXooX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import gensim.downloader as api\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pre-trained word embeddings\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")\n",
        "glove_vectors = api.load(\"glove-wiki-gigaword-300\")\n",
        "fasttext_vectors = api.load(\"fasttext-wiki-news-subwords-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the BiLSTM-CRF model\n",
        "class BiLSTMCRFTagger(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, word_embeddings):\n",
        "        super(BiLSTMCRFTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(word_embeddings, freeze=True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
        "        self.crf = CRF(tagset_size)\n",
        "    \n",
        "    def forward(self, sentence):\n",
        "        embeds = self.embedding(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        tag_space = self.hidden2tag(lstm_out)\n",
        "        return tag_space\n",
        "\n",
        "# Define the dataset\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, data_path, word_vectors, max_seq_length):\n",
        "        self.data_path = data_path\n",
        "        self.word_vectors = word_vectors\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.data = self.load_data()\n",
        "        self.word_to_idx, self.label_to_idx = self.prepare_vocab()\n",
        "        self.X, self.y = self.prepare_data()\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.data_path, 'r') as f:\n",
        "            data = json.load(f)\n",
        "        if isinstance(data, list):\n",
        "            data = {str(idx): sample for idx, sample in enumerate(data)}\n",
        "        return data\n",
        "\n",
        "    def prepare_vocab(self):\n",
        "        word_to_idx = {word: idx + 1 for idx, word in enumerate(self.word_vectors.index_to_key)}\n",
        "        label_to_idx = {\"O\": 1, \"B_RESPONDENT\": 2, \"I_RESPONDENT\": 3, \"B_DATE\": 4, \"I_DATE\":5, \"B_GPE\": 6, \"I_GPE\": 7, \"B_PROVISION\": 8, \n",
        "                        \"I_PROVISION\": 9, \"B_STATUTE\": 10, \"I_STATUTE\": 11, \"B_ORG\": 12, \"B_CASE_NUMBER\": 13, \"I_CASE_NUMBER\": 14, \n",
        "                        \"B_OTHER_PERSON\": 15, \"I_OTHER_PERSON\": 16, \"B_WITNESS\": 17, \"I_WITNESS\": 18, \"I_ORG\": 19, \"B_JUDGE\": 20, \"I_JUDGE\": 21, \n",
        "                        \"B_PETITIONER\": 22, \"I_PETITIONER\": 23, \"B_COURT\": 24, \"I_COURT\": 25, \"B_PRECEDENT\": 27, \"I_PRECEDENT\": 0}\n",
        "        return word_to_idx, label_to_idx\n",
        "\n",
        "    def prepare_data(self):\n",
        "        X, y = [], []\n",
        "        max_text_length = 0\n",
        "        max_label_length = 0\n",
        "\n",
        "        for sample_id, sample_data in self.data.items():\n",
        "            text = sample_data['text'].split()\n",
        "            labels = sample_data['labels']\n",
        "\n",
        "            text_indices = [self.word_to_idx.get(word, 0) for word in text]\n",
        "            label_indices = [self.label_to_idx[label] for label in labels]\n",
        "\n",
        "            X.append(torch.tensor(text_indices))\n",
        "            y.append(torch.tensor(label_indices))\n",
        "\n",
        "            max_text_length = max(max_text_length, len(text_indices))\n",
        "            max_label_length = max(max_label_length, len(label_indices))\n",
        "\n",
        "        # Pad both X and y to the maximum length\n",
        "        X_padded = pad_sequence(X, batch_first=True, padding_value=0)\n",
        "        y_padded = pad_sequence(y, batch_first=True, padding_value=0)\n",
        "\n",
        "        # Truncate to max_seq_length if necessary\n",
        "        X_padded = X_padded[:, :self.max_seq_length]\n",
        "        y_padded = y_padded[:, :self.max_seq_length]\n",
        "\n",
        "        # Pad or truncate X and y to the same length\n",
        "        if X_padded.size(1) > y_padded.size(1):\n",
        "            y_padded = torch.nn.functional.pad(y_padded, (0, X_padded.size(1) - y_padded.size(1)), value=0)\n",
        "        elif y_padded.size(1) > X_padded.size(1):\n",
        "            X_padded = torch.nn.functional.pad(X_padded, (0, y_padded.size(1) - X_padded.size(1)), value=0)\n",
        "\n",
        "        print(\"X_padded\", X_padded.shape)\n",
        "        print(\"Y_padded\", y_padded.shape)\n",
        "        return X_padded, y_padded\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Define the CRF layer\n",
        "class CRF(nn.Module):\n",
        "    def __init__(self, num_tags):\n",
        "        super(CRF, self).__init__()\n",
        "        self.num_tags = num_tags\n",
        "        self.transitions = nn.Parameter(torch.randn(num_tags, num_tags))\n",
        "\n",
        "    def forward(self, feats):\n",
        "        return feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZvAZoKP_XooY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_padded torch.Size([949, 100])\n",
            "Y_padded torch.Size([949, 100])\n",
            "X_padded torch.Size([949, 100])\n",
            "Y_padded torch.Size([949, 100])\n",
            "X_padded torch.Size([949, 100])\n",
            "Y_padded torch.Size([949, 100])\n"
          ]
        }
      ],
      "source": [
        "# Define hyperparameters\n",
        "max_seq_length = 100\n",
        "hidden_dim_word2vec = 125\n",
        "hidden_dim_glove = 256\n",
        "hidden_dim_fasttext = 125\n",
        "tagset_size = 28\n",
        "\n",
        "# Create data loaders\n",
        "test_dataset_word2vec = ReviewDataset('NER_test.json', word_vectors, max_seq_length)\n",
        "test_loader_word2vec = DataLoader(test_dataset_word2vec, batch_size=32, drop_last=True)\n",
        "test_dataset_glove = ReviewDataset('NER_test.json', glove_vectors, max_seq_length)\n",
        "test_loader_glove = DataLoader(test_dataset_glove, batch_size=32, drop_last=True)\n",
        "test_dataset_fasttext = ReviewDataset('NER_test.json', fasttext_vectors, max_seq_length)\n",
        "test_loader_fasttext = DataLoader(test_dataset_fasttext, batch_size=32, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2vec_state_dict = torch.load('t1_model4_word2vec.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_state_dict = torch.load('t1_model4_GloVe.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "fasttext_state_dict = torch.load('t1_model4_fasttext.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size_word2vec = len(word_vectors.index_to_key)\n",
        "vocab_size_glove = len(glove_vectors.index_to_key)\n",
        "vocab_size_fasttext = len(fasttext_vectors.index_to_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "word2vec_model = BiLSTMCRFTagger(word_vectors.vector_size, hidden_dim_word2vec, vocab_size_word2vec, tagset_size, torch.FloatTensor(word_vectors.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "glove_model = BiLSTMCRFTagger(glove_vectors.vector_size, hidden_dim_glove, vocab_size_glove, tagset_size, torch.FloatTensor(glove_vectors.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "fasttext_model = BiLSTMCRFTagger(fasttext_vectors.vector_size, hidden_dim_fasttext, vocab_size_fasttext, tagset_size, torch.FloatTensor(fasttext_vectors.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word2vec_model.load_state_dict(word2vec_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "glove_model.load_state_dict(glove_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "n_unAJqcXooZ",
        "outputId": "e19bdd67-34d9-412f-b428-ebf2fcad6dfa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fasttext_model.load_state_dict(fasttext_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    test_loss = 0\n",
        "    test_all_preds = []\n",
        "    test_all_labels = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for text, labels in test_loader:\n",
        "            outputs = model(text)\n",
        "            loss = criterion(outputs.view(-1, tagset_size), labels.view(-1))\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 2)\n",
        "            test_all_preds.extend(predicted.view(-1).cpu().numpy().tolist())\n",
        "            test_all_labels.extend(labels.view(-1).cpu().numpy().tolist())\n",
        "\n",
        "    # Calculate the accuracy and F1 score\n",
        "    test_f1 = f1_score(test_all_labels, test_all_preds, average='macro')\n",
        "    accuracy = accuracy_score(test_all_labels, test_all_preds)\n",
        "\n",
        "    return accuracy, test_f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "DnimKcfPXooa",
        "outputId": "6e1fac7d-ca41-4eb6-f0ec-b2220a165c3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T2 Model 4 Word2Vec Test Accuracy: 0.9566702586206897  Test F1 Score: 0.4625821460029647\n",
            "T2 Model 4 GloVe Test Accuracy: 0.9494827586206896  Test F1 Score: 0.3455169452643622\n",
            "T2 Model 4 Fasttext Test Accuracy: 0.9597198275862069  Test F1 Score: 0.4277152762183393\n"
          ]
        }
      ],
      "source": [
        "word2vec_accuracy, word2vec_f1 = evaluate_model(word2vec_model, test_loader_word2vec)\n",
        "glove_accuracy, glove_f1 = evaluate_model(glove_model, test_loader_glove)\n",
        "fasttext_accuracy, fasttext_f1 = evaluate_model(fasttext_model, test_loader_fasttext)\n",
        "\n",
        "# Print the results\n",
        "print(\"T2 Model 4 Word2Vec Test Accuracy:\", word2vec_accuracy, \" Test F1 Score:\", word2vec_f1)\n",
        "print(\"T2 Model 4 GloVe Test Accuracy:\", glove_accuracy, \" Test F1 Score:\", glove_f1)\n",
        "print(\"T2 Model 4 Fasttext Test Accuracy:\", fasttext_accuracy, \" Test F1 Score:\", fasttext_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
