{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"e14cb162-9921-41dc-9c40-09f9ef8e3484"},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import json\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dropout, Input, Embedding, Activation, Dense, LSTM, Conv1D, MaxPool1D\n","from keras.models import Sequential, Model\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import f1_score\n","import gensim.downloader as api\n","import matplotlib.pyplot as plt\n","from torch.nn.utils.rnn import pad_sequence"],"id":"e14cb162-9921-41dc-9c40-09f9ef8e3484"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"867b5d72-09b9-4ced-9258-01efb52ed3ae","outputId":"eb72d57f-4d95-438b-fbd1-a7ee04c97127","executionInfo":{"status":"ok","timestamp":1710162931863,"user_tz":-330,"elapsed":231458,"user":{"displayName":"Shruti Gupta","userId":"08034877041179821441"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 376.1/376.1MB downloaded\n"]}],"source":["glove_vectors = api.load(\"glove-wiki-gigaword-300\")"],"id":"867b5d72-09b9-4ced-9258-01efb52ed3ae"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"7007fc1e-dfb5-4cf2-9934-98a49f13b912","outputId":"3aa33a68-3754-41eb-f83f-e637171d59fd","executionInfo":{"status":"error","timestamp":1710165054649,"user_tz":-330,"elapsed":1853625,"user":{"displayName":"Shruti Gupta","userId":"08034877041179821441"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Training Loss: 19.3180, Validation Loss: 7.6536, Training Macro-F1: 0.0697, Validation Macro-F1: 0.0803\n","Epoch 2/10, Training Loss: 10.9270, Validation Loss: 6.3972, Training Macro-F1: 0.1070, Validation Macro-F1: 0.1315\n","Epoch 3/10, Training Loss: 10.0717, Validation Loss: 6.4037, Training Macro-F1: 0.1695, Validation Macro-F1: 0.1691\n","Epoch 4/10, Training Loss: 10.2175, Validation Loss: 6.7767, Training Macro-F1: 0.1853, Validation Macro-F1: 0.1527\n","Epoch 5/10, Training Loss: 13.5620, Validation Loss: 7.2479, Training Macro-F1: 0.1606, Validation Macro-F1: 0.0991\n","Epoch 6/10, Training Loss: 12.0603, Validation Loss: 7.0256, Training Macro-F1: 0.1293, Validation Macro-F1: 0.1579\n","Epoch 7/10, Training Loss: 11.2344, Validation Loss: 7.1419, Training Macro-F1: 0.1549, Validation Macro-F1: 0.1545\n","Epoch 8/10, Training Loss: 13.8146, Validation Loss: 6.9458, Training Macro-F1: 0.1389, Validation Macro-F1: 0.1347\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-01709c14b8e3>\u001b[0m in \u001b[0;36m<cell line: 127>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;31m# Generate plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-01709c14b8e3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Define the LSTM-based model\n","class LSTMTagger(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, word_embeddings):\n","        super(LSTMTagger, self).__init__()\n","        self.embedding = nn.Embedding.from_pretrained(word_embeddings, freeze=True)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        self.linear = nn.Linear(hidden_dim, tagset_size)\n","\n","    def forward(self, sentence):\n","        embeds = self.embedding(sentence)\n","        lstm_out, _ = self.lstm(embeds)\n","        tag_space = self.linear(lstm_out)\n","        return tag_space\n","\n","class ReviewDataset(Dataset):\n","    def __init__(self, data_path, glove_vectors, max_seq_length):\n","        self.data_path = data_path\n","        self.glove_vectors = glove_vectors\n","        self.max_seq_length = max_seq_length\n","        self.data = self.load_data()\n","        self.word_to_idx, self.label_to_idx = self.prepare_vocab()\n","        self.X, self.y = self.prepare_data()\n","\n","    def load_data(self):\n","        with open(self.data_path, 'r') as f:\n","            data = json.load(f)\n","        if isinstance(data, list):\n","            data = {str(idx): sample for idx, sample in enumerate(data)}\n","        return data\n","\n","    def prepare_vocab(self):\n","        word_to_idx = {word: idx + 1 for idx, word in enumerate(self.glove_vectors.index_to_key)}\n","        label_to_idx = {\"O\": 1, \"B_RESPONDENT\": 2, \"I_RESPONDENT\": 3, \"B_DATE\": 4, \"I_DATE\":5, \"B_GPE\": 6, \"I_GPE\": 7, \"B_PROVISION\": 8,\n","                \"I_PROVISION\": 9, \"B_STATUTE\": 10, \"I_STATUTE\": 11, \"B_ORG\": 12, \"B_CASE_NUMBER\": 13, \"I_CASE_NUMBER\": 14,\n","                \"B_OTHER_PERSON\": 15, \"I_OTHER_PERSON\": 16, \"B_WITNESS\": 17, \"I_WITNESS\": 18, \"I_ORG\": 19, \"B_JUDGE\": 20, \"I_JUDGE\": 21,\n","                \"B_PETITIONER\": 22, \"I_PETITIONER\": 23, \"B_COURT\": 24, \"I_COURT\": 25, \"B_PRECEDENT\": 26, \"I_PRECEDENT\": 0}\n","        return word_to_idx, label_to_idx\n","\n","    def prepare_data(self):\n","        X, y = [], []\n","        for sample in self.data.values():\n","            text = sample['text'].split(' ')\n","            labels = sample['labels']\n","            text_indices = [self.word_to_idx.get(word, 0) for word in text]\n","            X.append(torch.tensor(text_indices))\n","            y.append(torch.tensor([self.label_to_idx[label] for label in labels]))\n","        X_padded = pad_sequence(X, batch_first=True, padding_value=0)\n","        y_padded = pad_sequence(y, batch_first=True, padding_value=0)\n","        return X_padded, y_padded\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx], self.y[idx]\n","\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n","    train_losses = []\n","    val_losses = []\n","    train_f1s = []\n","    val_f1s = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","        train_all_preds = []\n","        train_all_labels = []\n","        for text, labels in train_loader:\n","            optimizer.zero_grad()\n","            outputs = model(text)\n","            loss = criterion(outputs.view(-1, tagset_size), labels.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 2)\n","            train_all_preds.extend(predicted.view(-1).cpu().numpy().tolist())\n","            train_all_labels.extend(labels.view(-1).cpu().numpy().tolist())\n","\n","        val_loss = 0\n","        val_all_preds = []\n","        val_all_labels = []\n","        with torch.no_grad():\n","            model.eval()\n","            for text, labels in val_loader:\n","                outputs = model(text)\n","                loss = criterion(outputs.view(-1, tagset_size), labels.view(-1))\n","                val_loss += loss.item()\n","\n","                _, predicted = torch.max(outputs, 2)\n","                val_all_preds.extend(predicted.view(-1).cpu().numpy().tolist())\n","                val_all_labels.extend(labels.view(-1).cpu().numpy().tolist())\n","\n","        train_losses.append(total_loss / len(train_loader))\n","        val_losses.append(val_loss / len(val_loader))\n","        train_f1 = f1_score(train_all_labels, train_all_preds, average='macro')\n","        val_f1 = f1_score(val_all_labels, val_all_preds, average='macro')\n","        train_f1s.append(train_f1)\n","        val_f1s.append(val_f1)\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss:.4f}, Validation Loss: {val_loss:.4f}, Training Macro-F1: {train_f1:.4f}, Validation Macro-F1: {val_f1:.4f}\")\n","\n","    return train_losses, val_losses, train_f1s, val_f1s\n","\n","max_seq_length = 100\n","\n","# Create data loaders\n","train_dataset = ReviewDataset('NER_TRAIN.json', glove_vectors, max_seq_length)\n","val_dataset = ReviewDataset('NER_VAL.json', glove_vectors, max_seq_length)\n","test_dataset = ReviewDataset('NER_TEST.json', glove_vectors, max_seq_length)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","# Define model parameters\n","embedding_dim = glove_vectors.vector_size\n","hidden_dim = 100\n","vocab_size = len(glove_vectors.index_to_key)\n","tagset_size = 28\n","\n","# Train the model\n","model = LSTMTagger(embedding_dim, hidden_dim, vocab_size, tagset_size, torch.FloatTensor(glove_vectors.vectors))\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.02)\n","\n","train_losses, val_losses, train_f1s, val_f1s = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10)\n","\n","# Generate plots\n","plt.figure(figsize=(12, 4))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Training Loss')\n","plt.plot(val_losses, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.title('Training and Validation Loss')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(train_f1s, label='Training Macro-F1')\n","plt.plot(val_f1s, label='Validation Macro-F1')\n","plt.xlabel('Epochs')\n","plt.ylabel('Macro-F1')\n","plt.legend()\n","plt.title('Training and Validation Macro-F1')\n","plt.show()\n","\n","# Save the model for future inference\n","torch.save(model.state_dict(), 't1_model2_gloVe.pt')\n","\n","# for 0.015 Epoch 8/10, Training Loss: 8.8402, Validation Loss: 5.8887, Training Macro-F1: 0.2400, Validation Macro-F1: 0.2321\n"],"id":"7007fc1e-dfb5-4cf2-9934-98a49f13b912"},{"cell_type":"code","execution_count":null,"metadata":{"id":"643b2940-5f9f-4a11-bc90-07ef9196d066"},"outputs":[],"source":[],"id":"643b2940-5f9f-4a11-bc90-07ef9196d066"}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":5}